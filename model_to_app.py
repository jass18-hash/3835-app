# -*- coding: utf-8 -*-
"""MODEL_TO_APP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDzjcNJYYg8AwEZYOFT8_dIMZlKDWc66

# GOAL: Predict whether a Saw-whet owl tagged by a specific tagging model will be recovered or not.


###
"""

#Block 1: Feature Review & Enhancement

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("Birds_cleaned.csv")

# Target: convert Y/N to 1/0
df["y"] = df["active__recovered"].map({"Y": 1, "N": 0}).fillna(0).astype(int)

# Handle rare or missing models
model_counts = df["active__model"].value_counts()
rare_models = model_counts[model_counts < 5].index
df["active__model"] = df["active__model"].replace(rare_models, "Other")

# One-hot encode the tagging model
df = pd.get_dummies(df, columns=["active__model"], prefix="model", dtype=int)

#scaling
scaler = StandardScaler()

print("Feature columns:", [c for c in df.columns if c.startswith("model_")])
print("Target distribution:\n", df["y"].value_counts(normalize=True))
df.head(10)

"""The feature set in this block was revisited and optimized, in accordance with the non-time-series classification problem of estimating owl recovery, using the tagging model.
The one-hot encoded categorical column active__model was created to be machine-readable and the uncommon categories were combined into one category, Other, to prevent sparsity.
The active column active__recovered was changed into binary (1 = Recovered, 0 = Not Recovered).
Since this data is independent tagging events and not sequential observations, there was no need in lag or Box-Cox transformation.
Training will control class imbalance through class weight balanced.
These delinquencies assure a clean dataset with balance and suitable to logistic or tree-based classification models.

# BLOCK 2
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

#Split into features and target
X = df[[c for c in df.columns if c.startswith("model_")]]
y = df["y"]

#Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

#1. Logistic Regression
log_reg = LogisticRegression(max_iter=500, class_weight="balanced", solver="liblinear")
cv_scores_lr = cross_val_score(log_reg, X_train, y_train, cv=cv, scoring="f1")
log_reg.fit(X_train, y_train)
y_pred_lr = log_reg.predict(X_test)
print("\n--- Logistic Regression ---")
print("Mean CV F1:", cv_scores_lr.mean())
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

#2. Random Forest
rf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)
cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=cv, scoring="f1")
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("\n--- Random Forest ---")
print("Mean CV F1:", cv_scores_rf.mean())
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

#3. Decision Tree
dt = DecisionTreeClassifier(class_weight="balanced", random_state=42)
cv_scores_dt = cross_val_score(dt, X_train, y_train, cv=cv, scoring="f1")
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print("\n--- Decision Tree ---")
print("Mean CV F1:", cv_scores_dt.mean())
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

#4. K-Nearest Neighbors
knn = KNeighborsClassifier(n_neighbors=5)
cv_scores_knn = cross_val_score(knn, X_train, y_train, cv=cv, scoring="f1")
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("\n--- K-Nearest Neighbors ---")
print("Mean CV F1:", cv_scores_knn.mean())
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))


#Compare all models in a simple table

import pandas as pd

# Collect metrics
results = {
    "Model": ["Logistic Regression", "Random Forest", "Decision Tree", "KNN"],
    "Mean CV F1": [
        cv_scores_lr.mean(),
        cv_scores_rf.mean(),
        cv_scores_dt.mean(),
        cv_scores_knn.mean()
    ],
    "Test Accuracy": [
        accuracy_score(y_test, y_pred_lr),
        accuracy_score(y_test, y_pred_rf),
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_knn)
    ],
}

# Create dataframe for comparison
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by="Test Accuracy", ascending=False).reset_index(drop=True)

print("\n--- Model Comparison Table ---")
print(results_df)

"""Logistic Regression and K-Nearest Neighbors (KNN) were the best performing out of all the four models.
Although KNN has the best accuracy (98.2%), its ROC-AUC (0.89) demonstrates that it is not as reliable in classifying.
The most balanced and trustworthy results are provided by Logistic Regression whose accuracy is high (94.6) and ROC-AUC is the greatest (0.95).
Thus, Logistic Regression is chosen as the last model, which is going to be optimized and further analyzed.

#BLOCK 3
"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score
import numpy as np

# Evaluate Logistic Regression
precision = precision_score(y_test, y_pred_lr)
recall = recall_score(y_test, y_pred_lr)
f1 = f1_score(y_test, y_pred_lr)
accuracy = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Evaluation Metrics")
print(f"Accuracy      : {accuracy:.3f}")
print(f"Precision     : {precision:.3f}")
print(f"Recall        : {recall:.3f}")
print(f"F1-score      : {f1:.3f}")

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc

# Confusion Matrix
ConfusionMatrixDisplay.from_estimator(log_reg, X_test, y_test, cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

compare_df = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest", "Decision Tree", "KNN"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_lr),
        accuracy_score(y_test, y_pred_rf),
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_knn)
    ],
    "F1-score": [
        f1_score(y_test, y_pred_lr),
        f1_score(y_test, y_pred_rf),
        f1_score(y_test, y_pred_dt),
        f1_score(y_test, y_pred_knn)
    ]
})

plt.figure(figsize=(8,5))
plt.plot(compare_df["Model"], compare_df["Accuracy"], marker='o', label="Accuracy", color='skyblue')
plt.plot(compare_df["Model"], compare_df["F1-score"], marker='s', label="F1-score", color='lightcoral')
plt.title("Model Comparison: Accuracy vs F1-score")
plt.xlabel("Model")
plt.ylabel("Score")
plt.ylim(0.4, 1.0)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.show()

models = ["Logistic Regression", "Random Forest", "Decision Tree", "KNN"]

accuracy = [
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_knn)
]

precision = [
    precision_score(y_test, y_pred_lr),
    precision_score(y_test, y_pred_rf),
    precision_score(y_test, y_pred_dt),
    precision_score(y_test, y_pred_knn)
]

recall = [
    recall_score(y_test, y_pred_lr),
    recall_score(y_test, y_pred_rf),
    recall_score(y_test, y_pred_dt),
    recall_score(y_test, y_pred_knn)
]

f1 = [
    f1_score(y_test, y_pred_lr),
    f1_score(y_test, y_pred_rf),
    f1_score(y_test, y_pred_dt),
    f1_score(y_test, y_pred_knn)
]

# Combine into a DataFrame
compare_df = pd.DataFrame({
    "Model": models,
    "Accuracy": accuracy,
    "Precision": precision,
    "Recall": recall,
    "F1-score": f1
})

# Plot all metrics in one line chart
plt.figure(figsize=(10,6))
plt.plot(compare_df["Model"], compare_df["Accuracy"], marker='o', label='Accuracy', color='green')
plt.plot(compare_df["Model"], compare_df["Precision"], marker='s', label='Precision', color='orange')
plt.plot(compare_df["Model"], compare_df["Recall"], marker='^', label='Recall', color='blue')
plt.plot(compare_df["Model"], compare_df["F1-score"], marker='D', label='F1-score', color='red')

plt.title("Model Performance Comparison (Accuracy, Precision, Recall, F1-score)")
plt.xlabel("Model")
plt.ylabel("Score")
plt.ylim(0.4, 1.0)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.show()

"""During this step, we tested the behavior of the Logistic Regression model with the most important classification measures — accuracy, precision, recall and F1-score.
The model had an accuracy of 94.6, precision of 0.54, recall of 0.86 and an F1-score of 0.66 which demonstrates a strong and balanced predictive performance.

According to the confusion matrix, the model was able to classify the rest of the non-recovered owls (true negatives = 1,499) and recovered owls (true positives = 90) with few misclassifications.

These comparison charts of all the models (Accuracy, Precision, Recall, and F1-score) show that even though KNN is a bit more accurate, the Logistic Regression model is more consistent and interpretable, which is what is required in this problem of classification.

On the whole, these findings prove the effectiveness, accuracy, and reliability of the Logistic Regression model in differentiating between the recovered and non-recovered owls on the basis of the tagging model history and the characteristics that are associated with this history.

#BLOCK 4
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score

# Simple parameter grid: only tuning regularization strength (C)
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100]   # small C = more regularization, large C = less regularization
}

# Base Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, class_weight='balanced')

# Grid Search
grid_search = GridSearchCV(
    estimator=log_reg,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# Best parameters and performance
print("Best C value:", grid_search.best_params_['C'])
print("Best Cross-Validation F1:", grid_search.best_score_)

#  Evaluate optimized model
best_log_reg = grid_search.best_estimator_
y_pred_best = best_log_reg.predict(X_test)

print("\nClassification Report (Optimized Logistic Regression):")
print(classification_report(y_test, y_pred_best))
print("Accuracy:", accuracy_score(y_test, y_pred_best))

"""The optimization of the Logistic Regression model was done in this block through GridSearchCV to optimize the regularization parameter (C).
C values (0.01-100) were experimented to identify an optimal balance between the model and overfitting.
C = 0.01 yielded the best result with great performance with:

Accuracy: 97.9%

Precision: 0.54 → 0.86

Recall: 0.86 → 0.79

F1-score: improved to 0.82

The optimized model demonstrates far greater balance of accuracy and recall, i.e. it recovers more owls with less false alarms.
Thus, the final tuned Logistic Regression model is very accurate, stable, and gives the most desirable overall results in making predictions concerning owl recovery outcomes.

#*Step 1: Enhance Model Explainability*


So, the model that we have used on this dataset is logistic regression and according to our problem statement we are using coefficients to show how each feature strongly affects the predicted outcome.

1. Positive coefficient increases the probability

2. Negative coefficient decreases the probability.

3. Absolute value of the coefficient represents its importance. Larger values means the feature has more influence on the prediction.
"""

# We are taking some colummns from X_train
feature_names = X_train.columns

# Extract coefficients from optimized model. It extracts all the best features that have good influence on predicition from the optimized model
coefficients = best_log_reg.coef_.ravel()

# Create importance dataframe
coef_df = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": coefficients,
    "Absolute Importance": np.abs(coefficients)
}).sort_values("Absolute Importance")

print("Coefficient-Based Feature Importance")
display(coef_df)

# --- Plot Top 10 Features ---
plt.figure(figsize=(8,6))
plt.barh(coef_df["Feature"].head(10)[::-1], coef_df["Absolute Importance"].head(10)[::-1], color="skyblue")
plt.title("Top 10 Most Important Features (Coefficient-Based)")
plt.xlabel("Absolute Coefficient Value")
plt.ylabel("Feature")
plt.grid(axis="x")
plt.show()

"""#**Insights & Explanation**

### So the main question is how it enhanced the model explainability ??

Our goal was to predict the model is going to be recovered or not by using information of models that was given.

What we did we found the coefficients of features. (point to be noted: its not a row or column, its a feature that influence the prediction.)

Our model was simple so using SHAP might resulted in complexity in explainability. so we used coefficient based feature importance.
"""

recovery_features = ["model_recovery_rate_overall", "model_month_recovery_rate"]

recovery_df = coef_df[coef_df["Feature"].isin(recovery_features)]
other_df = coef_df[~coef_df["Feature"].isin(recovery_features)]

# 1. BAR GRAPH for recovery-based features
plt.figure(figsize=(7,4))
plt.bar(recovery_df["Feature"], recovery_df["Coefficient"],
        color=["skyblue", "orange"], edgecolor="black")

plt.title("Comparison of Recovery Rate Features (Coefficient-Based Importance)")
plt.xlabel("Feature")
plt.ylabel("Coefficient Value")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12,5))
plt.plot(other_df["Feature"], other_df["Coefficient"],
         marker='o', linewidth=2, color="steelblue")
plt.title("All Other Model Features: Coefficient Line Graph")
plt.xlabel("Feature")
plt.ylabel("Coefficient Value")
plt.grid(True, linestyle="--", alpha=0.6)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()